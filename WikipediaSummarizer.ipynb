{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZc6N3vNlmQwCeeYk1YSiB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josh-Em/wikipedia-summarizer/blob/main/WikipediaSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üõ†Ô∏è Python Environment Setup with Libraries\n",
        "Install essential Python libraries: requests, BeautifulSoup, and OpenAI, for web scraping and API tasks."
      ],
      "metadata": {
        "id": "YIOHprQN2IpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 requests\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILZ6O23dymr3",
        "outputId": "c5176944-fe26-4754-fa66-b9ccc9e2bcc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ API Client Initialization with Python\n",
        "Import and set up requests, BeautifulSoup, and OpenAI libraries to enable web scraping and OpenAI API usage."
      ],
      "metadata": {
        "id": "wlfCAGLq2KCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = \"YOUR_API_KEY_HERE\"\n",
        "\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "ab8lULTg1335"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üåê Wikipedia Scraper Function\n",
        "Create a function to scrape and extract clean text from Wikipedia pag"
      ],
      "metadata": {
        "id": "DPc4koUC2KX0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVc89VlPkKi3"
      },
      "outputs": [],
      "source": [
        "def scrape_wikipedia(url):\n",
        "    \"\"\"\n",
        "    Scrapes text content from a Wikipedia page.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL of the Wikipedia page to scrape\n",
        "\n",
        "    Returns:\n",
        "        str: The main text content of the Wikipedia page\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the URL is not a valid Wikipedia page\n",
        "        requests.RequestException: If there's an error retrieving the page\n",
        "    \"\"\"\n",
        "    # Verify it's a Wikipedia URL\n",
        "    if not url.startswith('https://en.wikipedia.org/wiki/'):\n",
        "        raise ValueError(\"URL must be a valid English Wikipedia page\")\n",
        "\n",
        "    try:\n",
        "        # Send GET request to the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Create BeautifulSoup object to parse HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove unwanted elements\n",
        "        for element in soup.find_all(['script', 'style', 'table', 'sup', 'span']):\n",
        "            element.decompose()\n",
        "\n",
        "        # Get the main content div\n",
        "        content = soup.find(id='mw-content-text')\n",
        "        if not content:\n",
        "            raise ValueError(\"Could not find main content section\")\n",
        "\n",
        "        # Get all text from the main content\n",
        "        text = content.get_text(separator=' ')\n",
        "\n",
        "        # Clean up the text\n",
        "        text = re.sub(r'\\[\\d+\\]', '', text)  # Remove reference numbers\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        raise requests.RequestException(f\"Error retrieving the page: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù Wikipedia Article Summarizer with GPT\n",
        "Utilize OpenAI's GPT to generate concise summaries of Wikipedia articles by processing the extrac"
      ],
      "metadata": {
        "id": "MgbPy5A32K5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_summary(wiki_text):\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"You are a wikipedia page summarizer. Given a wikipedia article, please summarize it in two or three sentences.\"\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": wiki_text\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    response_format={\n",
        "      \"type\": \"text\"\n",
        "    },\n",
        "    temperature=1,\n",
        "    max_completion_tokens=2048,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "adprexDSyrZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç Scrape and Summarize Wikipedia Article\n",
        "Scrape a Wikipedia page and generate a brief summary using predefined functions for content extraction and summarization."
      ],
      "metadata": {
        "id": "hK6B0Oy12Lck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Mesopotamia\"\n",
        "text = scrape_wikipedia(url)\n",
        "summary = get_summary(text)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "359yN21r1Co3",
        "outputId": "703679dd-6d24-4326-9e78-91a35d6eb1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mesopotamia, known as the \"land between rivers,\" is a historical region in West Asia, primarily within present-day Iraq, but also covering parts of Iran, Turkey, Syria, and Kuwait. It is recognized as the cradle of civilization for its significant contributions, including the development of the first cereal crops, the invention of the wheel, and the establishment of the earliest forms of writing and mathematics. Its complex history saw the rise and fall of empires such as the Sumerians, Akkadians, Babylonians, and Assyrians, influencing the region until the Muslim conquests in the 7th century AD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTS2IU0w1swn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}